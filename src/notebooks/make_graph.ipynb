{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Redo graph.pkl",
   "id": "30b101bff87aff3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T11:53:30.422297Z",
     "start_time": "2025-09-01T11:53:11.565531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inspector_git import IGLogReader, GitLogDTO\n",
    "from src.jira_miner.models import JsonFileFormatJira\n",
    "from src.github_miner import JsonFileFormatGithub\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from graph import *\n",
    "\n",
    "# JSON\n",
    "path_jira = \"/home/alex/Work/BachelorThesis/Vortex/test-input/jira-miner/ZEPPELIN-detailed-issues.json\"\n",
    "path_github = \"/home/alex/Work/BachelorThesis/Vortex/test-input/github-miner/githubProject.json\"\n",
    "# IGLOG\n",
    "path_inspector_git = \"/home/alex/Work/BachelorThesis/Vortex/test-input/inspector-git/zeppelin.iglog\"\n",
    "\n",
    "\n",
    "\n",
    "def load_from_json(model_cls, file_path: str):\n",
    "    \"\"\"Load JSON from a given path and validate it with the provided model class.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return model_cls.model_validate(data)\n",
    "\n",
    "def load_jsons():\n",
    "    jira_data_loaded = load_from_json(JsonFileFormatJira, path_jira)\n",
    "    github_data_loaded = load_from_json(JsonFileFormatGithub, path_github)\n",
    "    return jira_data_loaded, github_data_loaded\n",
    "\n",
    "IGLogReader = IGLogReader()\n",
    "\n",
    "\n",
    "\n",
    "jira_data, github_data = load_jsons()\n",
    "inspector_git_data = IGLogReader.read(path_inspector_git)\n",
    "graph = Graph()\n",
    "\n",
    "def add_inspector_git_data(graph:Graph, inspector_git_data: GitLogDTO):\n",
    "    git_date_format = \"%a %b %d %H:%M:%S %Y %z\"\n",
    "\n",
    "    for commitDTO in inspector_git_data.commits:\n",
    "        commit = GitCommit(\n",
    "            sha=commitDTO.id,\n",
    "            message=commitDTO.message,\n",
    "            author_date=datetime.strptime(commitDTO.author_date, git_date_format),\n",
    "            committer_date=datetime.strptime(commitDTO.committer_date, git_date_format)\n",
    "        )\n",
    "        graph.add_commit(commit)\n",
    "\n",
    "        author = GitUser(email=commitDTO.author_email, name=commitDTO.author_name)\n",
    "        committer = GitUser(email=commitDTO.committer_email, name=commitDTO.committer_name)\n",
    "        graph.add_user_git(author)\n",
    "        graph.add_user_git(committer)\n",
    "\n",
    "        author_edge = GitCommitGitUserEdge(commit=commit, git_user=author, role=\"author\")\n",
    "        committer_edge = GitCommitGitUserEdge(commit=commit, git_user=committer, role=\"committer\")\n",
    "        graph.add_edge(author_edge)\n",
    "        graph.add_edge(committer_edge)\n",
    "\n",
    "        for change in commitDTO.changes:\n",
    "            file = File(path = change.new_file_name)\n",
    "            graph.add_file(file = file , old_name = change.old_file_name)\n",
    "\n",
    "            file_commit_edge = GitCommitFileEdge(commit=commit, file=file)\n",
    "            file_writer_edge = GitUserFileEdge(git_user=committer, file=file, role=\"writer\")\n",
    "            graph.add_edge(file_commit_edge)\n",
    "            graph.add_edge(file_writer_edge)\n",
    "\n",
    "            if author.email != committer.email:\n",
    "                file_reviewer_edge = GitUserFileEdge(git_user=author, file=file, role=\"reviewer\")\n",
    "                graph.add_edge(file_reviewer_edge)\n",
    "\n",
    "add_inspector_git_data(graph, inspector_git_data)\n",
    "\n",
    "def add_jira_data(graph:Graph, jira_data: JsonFileFormatJira):\n",
    "    def add_issue_statuses():\n",
    "        for status in jira_data.issueStatuses:\n",
    "            category = IssueStatusCategory(\n",
    "                key=status.statusCategory.key,\n",
    "                name=status.statusCategory.name\n",
    "            )\n",
    "            status = IssueStatus(\n",
    "                id=status.id,\n",
    "                name=status.name,\n",
    "            )\n",
    "\n",
    "            graph.add_issue_status_category(category)\n",
    "            graph.add_issue_status(status)\n",
    "\n",
    "            edge = IssueStatusIssueStatusCategoryEdge(\n",
    "                issue_status=status,\n",
    "                issue_status_category=category,\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "    def add_issue_types():\n",
    "        for issue_type in jira_data.issueTypes:\n",
    "            issue_type = IssueType(\n",
    "                id=issue_type.id,\n",
    "                name=issue_type.name,\n",
    "                description=issue_type.description,\n",
    "                isSubTask=issue_type.isSubTask,\n",
    "            )\n",
    "            graph.add_issue_type(issue_type)\n",
    "\n",
    "    def add_users():\n",
    "        for user in jira_data.users:\n",
    "            jira_user = JiraUser(\n",
    "                key=user.key,\n",
    "                name=user.name,\n",
    "                link=user.self_,\n",
    "            )\n",
    "            graph.add_jira_user(jira_user)\n",
    "\n",
    "    def add_issues():\n",
    "        for issue in jira_data.issues:\n",
    "            i = Issue(\n",
    "                id=issue.id,\n",
    "                key=issue.key,\n",
    "                summary=issue.summary,\n",
    "                createdAt=issue.created,\n",
    "                updatedAt=issue.updated,\n",
    "            )\n",
    "            graph.add_issue(i)\n",
    "\n",
    "            issue_status = graph.get_issue_status(issue.status.id)\n",
    "            issue_issue_status_edge = IssueIssueStatusEdge(\n",
    "                issue_status=issue_status,\n",
    "                issue=i\n",
    "            )\n",
    "            graph.add_edge(issue_issue_status_edge)\n",
    "\n",
    "            issue_type = graph.get_issue_type(issue.issueType)\n",
    "            issue_issue_type_edge = IssueIssueTypeEdge(\n",
    "                issue_type=issue_type,\n",
    "                issue=i\n",
    "            )\n",
    "            graph.add_edge(issue_issue_type_edge)\n",
    "\n",
    "            reporter = graph.get_jira_user(issue.reporterId)\n",
    "            reporter_edge = IssueJiraUserEdge(\n",
    "                jira_user=reporter,\n",
    "                issue=i,\n",
    "                role= \"reporter\"\n",
    "            )\n",
    "            graph.add_edge(reporter_edge)\n",
    "            if issue.creatorId is not None:\n",
    "                creator = graph.get_jira_user(issue.creatorId)\n",
    "                creator_edge = IssueJiraUserEdge(\n",
    "                jira_user=creator,\n",
    "                issue=i,\n",
    "                role= \"creator\"\n",
    "                )\n",
    "                graph.add_edge(creator_edge)\n",
    "\n",
    "            if issue.assigneeId is not None:\n",
    "                assignee = graph.get_jira_user(issue.assigneeId)\n",
    "                assignee_edge = IssueJiraUserEdge(\n",
    "                    jira_user=assignee,\n",
    "                    issue=i,\n",
    "                    role= \"assignee\"\n",
    "                )\n",
    "                graph.add_edge(assignee_edge)\n",
    "\n",
    "    def add_edge_if_absent(graph, edge: IssueIssueEdge):\n",
    "        \"\"\"Add an IssueIssueEdge only if it doesn't already exist.\"\"\"\n",
    "        existing_edges = graph.adjacency.get(edge.child.dict_key(), {}).get(\"issues\", [])\n",
    "        if not any(\n",
    "            isinstance(e, IssueIssueEdge) and e.normalized_key() == edge.normalized_key()\n",
    "            for e in existing_edges\n",
    "        ):\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "    def make_issue_parent_connections():\n",
    "        for jira_issue in jira_data.issues:\n",
    "            current_issue = graph.get_issue(jira_issue.key)\n",
    "\n",
    "            # ðŸ”¹ Connect to parent\n",
    "            if jira_issue.parent is not None:\n",
    "                parent_issue = graph.get_issue(jira_issue.parent)\n",
    "                edge = IssueIssueEdge(child=current_issue, parent=parent_issue)\n",
    "                add_edge_if_absent(graph, edge)\n",
    "\n",
    "            # ðŸ”¹ Connect to subtasks\n",
    "            for subtask_id in jira_issue.subTasks or []:\n",
    "                child_issue = graph.get_issue(subtask_id)\n",
    "                edge = IssueIssueEdge(child=child_issue, parent=current_issue)\n",
    "                add_edge_if_absent(graph, edge)\n",
    "\n",
    "\n",
    "    add_issue_statuses()\n",
    "    add_issue_types()\n",
    "    add_users()\n",
    "    add_issues()\n",
    "    make_issue_parent_connections()\n",
    "\n",
    "add_jira_data(graph, jira_data)\n",
    "\n",
    "def add_github_data(graph:Graph, github_data: JsonFileFormatGithub):\n",
    "    a = 0\n",
    "    cre = 0\n",
    "    m = 0\n",
    "    for pr in github_data.pullRequests:\n",
    "        # ADD PR\n",
    "        pull_request = PullRequest(\n",
    "            number=pr.number,\n",
    "            title=pr.title,\n",
    "            state=pr.state,\n",
    "            changedFiles=pr.changedFiles,\n",
    "            createdAt=pr.createdAt,\n",
    "            updatedAt=pr.updatedAt,\n",
    "            body=pr.body,\n",
    "            mergedAt=pr.mergedAt,\n",
    "            closedAt=pr.closedAt,\n",
    "        )\n",
    "        graph.add_pull_request(pull_request)\n",
    "\n",
    "        # ADD ALL USERS\n",
    "        for assignee in pr.assignees:\n",
    "            assignee_git_hub_user = GitHubUser(\n",
    "                url=assignee.url,\n",
    "                login=assignee.login,\n",
    "                name=assignee.name,\n",
    "            )\n",
    "            graph.add_git_hub_user(assignee_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= assignee_git_hub_user,\n",
    "                role = \"assignee\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            a += 1\n",
    "\n",
    "        if pr.createdBy:\n",
    "            creator_git_hub_user = GitHubUser(\n",
    "                url=pr.createdBy.url,\n",
    "                login=pr.createdBy.login,\n",
    "                name=pr.createdBy.name,\n",
    "            )\n",
    "            graph.add_git_hub_user(creator_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= creator_git_hub_user,\n",
    "                role = \"creator\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            cre += 1\n",
    "\n",
    "        if pr.mergedBy:\n",
    "            merger_git_hub_user = GitHubUser(\n",
    "                name=pr.mergedBy.name,\n",
    "                url=pr.mergedBy.url,\n",
    "                login=pr.mergedBy.login,\n",
    "            )\n",
    "            graph.add_git_hub_user(merger_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= merger_git_hub_user,\n",
    "                role = \"merger\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            m += 1\n",
    "\n",
    "        # ADD ALL COMMITS\n",
    "        for c in pr.commits:\n",
    "            commit = GitHubCommit(\n",
    "                sha=c.sha,\n",
    "                date=c.date,\n",
    "                message=c.message,\n",
    "                changedFiles=c.changedFiles,\n",
    "            )\n",
    "            graph.add_git_hub_commit(commit)\n",
    "\n",
    "            edge = PullRequestGitHubCommitEdge(\n",
    "                commit = commit,\n",
    "                pr = pull_request,\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "add_github_data(graph, github_data)\n",
    "\n",
    "def save_pickle(obj, var_name: str, base_dir: str = \"pickle_data\") -> Path:\n",
    "    \"\"\"\n",
    "    Save a Python object using pickle in a directory relative to the notebook.\n",
    "\n",
    "    Args:\n",
    "        obj: The Python object to save.\n",
    "        var_name: Name of the variable (used as filename).\n",
    "        base_dir: Relative directory to save pickles (default: \"pickle_data\").\n",
    "\n",
    "    Returns:\n",
    "        Path to the saved pickle file.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    save_dir = Path(base_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build full path\n",
    "    pickle_path = save_dir / f\"{var_name}.pkl\"\n",
    "\n",
    "    # Save object\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "    print(f\"Saved {var_name} to {pickle_path}\")\n",
    "    return pickle_path\n",
    "\n",
    "# Save your loaded data\n",
    "save_pickle(graph, \"graph\")"
   ],
   "id": "bfe9b65ecef11a51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved graph to pickle_data/graph.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('pickle_data/graph.pkl')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "6cf802031df5a11b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-02T12:20:12.780062Z",
     "start_time": "2025-09-02T12:20:12.697862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inspector_git import IGLogReader, GitLogDTO\n",
    "from src.jira_miner.models import JsonFileFormatJira\n",
    "from src.github_miner import JsonFileFormatGithub\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from graph import *"
   ],
   "id": "19b4f43a6de7b5fc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T12:20:14.530372Z",
     "start_time": "2025-09-02T12:20:13.261671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(pickle_file: str):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "graph = load_pickle(\"pickle_data/graph.pkl\")\n",
    "\n",
    "# Quick check\n",
    "print(\"Graph data type:\", type(graph))\n",
    "print(graph)"
   ],
   "id": "102cf6749bb10755",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data type: <class 'graph.Graph'>\n",
      "~~~~ Graph summary ~~~~\n",
      "commits: 5512\n",
      "git_users: 602\n",
      "files: 5644\n",
      "\n",
      "issue_statuses: 48\n",
      "issue_types: 7\n",
      "issue_status_categories: 3\n",
      "jira_users: 2008\n",
      "issues: 6202\n",
      "\n",
      "pull_requests: 5022\n",
      "git_hub_users: 690\n",
      "git_hub_commits: 17869\n",
      "\n",
      "edges: 177967\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data from SERIALIZED formats",
   "id": "ca523fd94252c3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:42.094177Z",
     "start_time": "2025-09-02T11:39:23.252373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON\n",
    "path_jira = \"/home/vortex/Work/BachelorThesis/Vortex/test-input/jira-miner/ZEPPELIN-detailed-issues.json\"\n",
    "path_github = \"/home/vortex/Work/BachelorThesis/Vortex/test-input/github-miner/githubProject.json\"\n",
    "# IGLOG\n",
    "path_inspector_git = \"/home/vortex/Work/BachelorThesis/Vortex/test-input/inspector-git/zeppelin.iglog\"\n",
    "\n",
    "\n",
    "\n",
    "def load_from_json(model_cls, file_path: str):\n",
    "    \"\"\"Load JSON from a given path and validate it with the provided model class.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return model_cls.model_validate(data)\n",
    "\n",
    "def load_jsons():\n",
    "    jira_data_loaded = load_from_json(JsonFileFormatJira, path_jira)\n",
    "    github_data_loaded = load_from_json(JsonFileFormatGithub, path_github)\n",
    "    return jira_data_loaded, github_data_loaded\n",
    "\n",
    "IGLogReader = IGLogReader()\n",
    "\n",
    "\n",
    "\n",
    "jira_data, github_data = load_jsons()\n",
    "inspector_git_data = IGLogReader.read(path_inspector_git)\n",
    "graph = Graph()"
   ],
   "id": "d85345729030199f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add data from inspector Git",
   "id": "3af611336e9a011b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T11:44:29.306641Z",
     "start_time": "2025-08-29T11:44:28.831645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_inspector_git_data(graph:Graph, inspector_git_data: GitLogDTO):\n",
    "    git_date_format = \"%a %b %d %H:%M:%S %Y %z\"\n",
    "\n",
    "    for commitDTO in inspector_git_data.commits:\n",
    "        commit = GitCommit(\n",
    "            sha=commitDTO.id,\n",
    "            message=commitDTO.message,\n",
    "            author_date=datetime.strptime(commitDTO.author_date, git_date_format),\n",
    "            committer_date=datetime.strptime(commitDTO.committer_date, git_date_format)\n",
    "        )\n",
    "        graph.add_commit(commit)\n",
    "\n",
    "        author = GitUser(email=commitDTO.author_email, name=commitDTO.author_name)\n",
    "        committer = GitUser(email=commitDTO.committer_email, name=commitDTO.committer_name)\n",
    "        graph.add_user_git(author)\n",
    "        graph.add_user_git(committer)\n",
    "\n",
    "        author_edge = GitCommitGitUserEdge(commit=commit, git_user=author, role=\"author\")\n",
    "        committer_edge = GitCommitGitUserEdge(commit=commit, git_user=committer, role=\"committer\")\n",
    "        graph.add_edge(author_edge)\n",
    "        graph.add_edge(committer_edge)\n",
    "\n",
    "        for change in commitDTO.changes:\n",
    "            file = File(path = change.new_file_name)\n",
    "            graph.add_file(file = file , old_name = change.old_file_name)\n",
    "\n",
    "            file_commit_edge = GitCommitFileEdge(commit=commit, file=file)\n",
    "            file_writer_edge = GitUserFileEdge(git_user=committer, file=file, role=\"writer\")\n",
    "            graph.add_edge(file_commit_edge)\n",
    "            graph.add_edge(file_writer_edge)\n",
    "\n",
    "            if author.email != committer.email:\n",
    "                file_reviewer_edge = GitUserFileEdge(git_user=author, file=file, role=\"reviewer\")\n",
    "                graph.add_edge(file_reviewer_edge)\n",
    "\n",
    "add_inspector_git_data(graph, inspector_git_data)\n",
    "\n",
    "print(graph)"
   ],
   "id": "8f6593d9560f0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ Graph summary ~~~~\n",
      "commits: 5512\n",
      "git_users: 602\n",
      "files: 5644\n",
      "\n",
      "issue_statuses: 0\n",
      "issue_types: 0\n",
      "issue_status_categories: 0\n",
      "jira_users: 0\n",
      "issues: 0\n",
      "\n",
      "pull_requests: 0\n",
      "git_hub_users: 0\n",
      "git_hub_commits: 0\n",
      "\n",
      "edges: 100670\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding data from Jira to the graph",
   "id": "ed4e183a24fe1559"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T11:44:33.302563Z",
     "start_time": "2025-08-29T11:44:32.995113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_jira_data(graph:Graph, jira_data: JsonFileFormatJira):\n",
    "    def add_issue_statuses():\n",
    "        for status in jira_data.issueStatuses:\n",
    "            category = IssueStatusCategory(\n",
    "                key=status.statusCategory.key,\n",
    "                name=status.statusCategory.name\n",
    "            )\n",
    "            status = IssueStatus(\n",
    "                id=status.id,\n",
    "                name=status.name,\n",
    "            )\n",
    "\n",
    "            graph.add_issue_status_category(category)\n",
    "            graph.add_issue_status(status)\n",
    "\n",
    "            edge = IssueStatusIssueStatusCategoryEdge(\n",
    "                issue_status=status,\n",
    "                issue_status_category=category,\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "    def add_issue_types():\n",
    "        for issue_type in jira_data.issueTypes:\n",
    "            issue_type = IssueType(\n",
    "                id=issue_type.id,\n",
    "                name=issue_type.name,\n",
    "                description=issue_type.description,\n",
    "                isSubTask=issue_type.isSubTask,\n",
    "            )\n",
    "            graph.add_issue_type(issue_type)\n",
    "\n",
    "    def add_users():\n",
    "        for user in jira_data.users:\n",
    "            jira_user = JiraUser(\n",
    "                key=user.key,\n",
    "                name=user.name,\n",
    "                link=user.self_,\n",
    "            )\n",
    "            graph.add_jira_user(jira_user)\n",
    "\n",
    "    def add_issues():\n",
    "        for issue in jira_data.issues:\n",
    "            i = Issue(\n",
    "                id=issue.id,\n",
    "                key=issue.key,\n",
    "                summary=issue.summary,\n",
    "                createdAt=issue.created,\n",
    "                updatedAt=issue.updated,\n",
    "            )\n",
    "            graph.add_issue(i)\n",
    "\n",
    "            issue_status = graph.get_issue_status(issue.status.id)\n",
    "            issue_issue_status_edge = IssueIssueStatusEdge(\n",
    "                issue_status=issue_status,\n",
    "                issue=i\n",
    "            )\n",
    "            graph.add_edge(issue_issue_status_edge)\n",
    "\n",
    "            issue_type = graph.get_issue_type(issue.issueType)\n",
    "            issue_issue_type_edge = IssueIssueTypeEdge(\n",
    "                issue_type=issue_type,\n",
    "                issue=i\n",
    "            )\n",
    "            graph.add_edge(issue_issue_type_edge)\n",
    "\n",
    "            reporter = graph.get_jira_user(issue.reporterId)\n",
    "            reporter_edge = IssueJiraUserEdge(\n",
    "                jira_user=reporter,\n",
    "                issue=i,\n",
    "                role= \"reporter\"\n",
    "            )\n",
    "            graph.add_edge(reporter_edge)\n",
    "            if issue.creatorId is not None:\n",
    "                creator = graph.get_jira_user(issue.creatorId)\n",
    "                creator_edge = IssueJiraUserEdge(\n",
    "                jira_user=creator,\n",
    "                issue=i,\n",
    "                role= \"creator\"\n",
    "                )\n",
    "                graph.add_edge(creator_edge)\n",
    "\n",
    "            if issue.assigneeId is not None:\n",
    "                assignee = graph.get_jira_user(issue.assigneeId)\n",
    "                assignee_edge = IssueJiraUserEdge(\n",
    "                    jira_user=assignee,\n",
    "                    issue=i,\n",
    "                    role= \"assignee\"\n",
    "                )\n",
    "                graph.add_edge(assignee_edge)\n",
    "\n",
    "    def add_edge_if_absent(graph, edge: IssueIssueEdge):\n",
    "        \"\"\"Add an IssueIssueEdge only if it doesn't already exist.\"\"\"\n",
    "        existing_edges = graph.adjacency.get(edge.child.dict_key(), {}).get(\"issues\", [])\n",
    "        if not any(\n",
    "            isinstance(e, IssueIssueEdge) and e.normalized_key() == edge.normalized_key()\n",
    "            for e in existing_edges\n",
    "        ):\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "    def make_issue_parent_connections():\n",
    "        for jira_issue in jira_data.issues:\n",
    "            current_issue = graph.get_issue(jira_issue.key)\n",
    "\n",
    "            # ðŸ”¹ Connect to parent\n",
    "            if jira_issue.parent is not None:\n",
    "                parent_issue = graph.get_issue(jira_issue.parent)\n",
    "                edge = IssueIssueEdge(child=current_issue, parent=parent_issue)\n",
    "                add_edge_if_absent(graph, edge)\n",
    "\n",
    "            # ðŸ”¹ Connect to subtasks\n",
    "            for subtask_id in jira_issue.subTasks or []:\n",
    "                child_issue = graph.get_issue(subtask_id)\n",
    "                edge = IssueIssueEdge(child=child_issue, parent=current_issue)\n",
    "                add_edge_if_absent(graph, edge)\n",
    "\n",
    "\n",
    "    add_issue_statuses()\n",
    "    add_issue_types()\n",
    "    add_users()\n",
    "    add_issues()\n",
    "    make_issue_parent_connections()\n",
    "\n",
    "add_jira_data(graph, jira_data)\n",
    "\n",
    "print(graph)"
   ],
   "id": "2bebf79e505e8db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ Graph summary ~~~~\n",
      "commits: 5512\n",
      "git_users: 602\n",
      "files: 5644\n",
      "\n",
      "issue_statuses: 48\n",
      "issue_types: 7\n",
      "issue_status_categories: 3\n",
      "jira_users: 2008\n",
      "issues: 6202\n",
      "\n",
      "pull_requests: 0\n",
      "git_hub_users: 0\n",
      "git_hub_commits: 0\n",
      "\n",
      "edges: 129581\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T20:31:22.336641Z",
     "start_time": "2025-08-26T20:31:22.333571Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Add data from GitHub",
   "id": "9341d45ca4137141"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T11:44:39.338408Z",
     "start_time": "2025-08-29T11:44:39.132014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_github_data(graph:Graph, github_data: JsonFileFormatGithub):\n",
    "    a = 0\n",
    "    cre = 0\n",
    "    m = 0\n",
    "    for pr in github_data.pullRequests:\n",
    "        # ADD PR\n",
    "        pull_request = PullRequest(\n",
    "            number=pr.number,\n",
    "            title=pr.title,\n",
    "            state=pr.state,\n",
    "            changedFiles=pr.changedFiles,\n",
    "            createdAt=pr.createdAt,\n",
    "            updatedAt=pr.updatedAt,\n",
    "            body=pr.body,\n",
    "            mergedAt=pr.mergedAt,\n",
    "            closedAt=pr.closedAt,\n",
    "        )\n",
    "        graph.add_pull_request(pull_request)\n",
    "\n",
    "        # ADD ALL USERS\n",
    "        for assignee in pr.assignees:\n",
    "            assignee_git_hub_user = GitHubUser(\n",
    "                url=assignee.url,\n",
    "                login=assignee.login,\n",
    "                name=assignee.name,\n",
    "            )\n",
    "            graph.add_git_hub_user(assignee_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= assignee_git_hub_user,\n",
    "                role = \"assignee\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            a += 1\n",
    "\n",
    "        if pr.createdBy:\n",
    "            creator_git_hub_user = GitHubUser(\n",
    "                url=pr.createdBy.url,\n",
    "                login=pr.createdBy.login,\n",
    "                name=pr.createdBy.name,\n",
    "            )\n",
    "            graph.add_git_hub_user(creator_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= creator_git_hub_user,\n",
    "                role = \"creator\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            cre += 1\n",
    "\n",
    "        if pr.mergedBy:\n",
    "            merger_git_hub_user = GitHubUser(\n",
    "                name=pr.mergedBy.name,\n",
    "                url=pr.mergedBy.url,\n",
    "                login=pr.mergedBy.login,\n",
    "            )\n",
    "            graph.add_git_hub_user(merger_git_hub_user)\n",
    "\n",
    "            edge = PullRequestGitHubUserEdge(\n",
    "                pr = pull_request,\n",
    "                git_hub_user= merger_git_hub_user,\n",
    "                role = \"merger\"\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            m += 1\n",
    "\n",
    "        # ADD ALL COMMITS\n",
    "        for c in pr.commits:\n",
    "            commit = GitHubCommit(\n",
    "                sha=c.sha,\n",
    "                date=c.date,\n",
    "                message=c.message,\n",
    "                changedFiles=c.changedFiles,\n",
    "            )\n",
    "            graph.add_git_hub_commit(commit)\n",
    "\n",
    "            edge = PullRequestGitHubCommitEdge(\n",
    "                commit = commit,\n",
    "                pr = pull_request,\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "    print(\"assignees:\", a)\n",
    "    print(\"creators:\", cre)\n",
    "    print(\"mergers:\", m)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "add_github_data(graph, github_data)\n",
    "\n",
    "print(graph)"
   ],
   "id": "a3fe07087637b8a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignees: 60\n",
      "creators: 5003\n",
      "mergers: 488\n",
      "\n",
      "\n",
      "\n",
      "~~~~ Graph summary ~~~~\n",
      "commits: 5512\n",
      "git_users: 602\n",
      "files: 5644\n",
      "\n",
      "issue_statuses: 48\n",
      "issue_types: 7\n",
      "issue_status_categories: 3\n",
      "jira_users: 2008\n",
      "issues: 6202\n",
      "\n",
      "pull_requests: 5022\n",
      "git_hub_users: 690\n",
      "git_hub_commits: 17869\n",
      "\n",
      "edges: 155428\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:34:53.132662Z",
     "start_time": "2025-08-27T11:34:53.130139Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Save graph with pickle",
   "id": "ef679c14c84d1cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T11:52:41.097834Z",
     "start_time": "2025-09-01T11:52:39.907015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_pickle(obj, var_name: str, base_dir: str = \"pickle_data\") -> Path:\n",
    "    \"\"\"\n",
    "    Save a Python object using pickle in a directory relative to the notebook.\n",
    "\n",
    "    Args:\n",
    "        obj: The Python object to save.\n",
    "        var_name: Name of the variable (used as filename).\n",
    "        base_dir: Relative directory to save pickles (default: \"pickle_data\").\n",
    "\n",
    "    Returns:\n",
    "        Path to the saved pickle file.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    save_dir = Path(base_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build full path\n",
    "    pickle_path = save_dir / f\"{var_name}.pkl\"\n",
    "\n",
    "    # Save object\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "    print(f\"Saved {var_name} to {pickle_path}\")\n",
    "    return pickle_path\n",
    "\n",
    "# Save your loaded data\n",
    "save_pickle(graph, \"graph\")"
   ],
   "id": "6df56d6cfeb8ce3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved graph_safe to pickle_data/graph_safe.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('pickle_data/graph_safe.pkl')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load graph",
   "id": "c19469a906e664d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make graph fully connected",
   "id": "6cfcf28d371eb05e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T12:02:49.223021Z",
     "start_time": "2025-09-01T11:54:08.711641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "from difflib import SequenceMatcher\n",
    "import bisect\n",
    "\n",
    "def link_issues_with_git_commits(graph: Graph):\n",
    "    # Build one regex to match all issue keys\n",
    "    issue_keys = [re.escape(issue.key) for issue in graph.issues.values()]\n",
    "    issue_pattern = re.compile(r'\\b(' + '|'.join(issue_keys) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    links = 0\n",
    "    commits_liked_with_issues = 0\n",
    "    for commit in graph.commits.values():\n",
    "        if not commit.message:\n",
    "            continue\n",
    "\n",
    "        matches = issue_pattern.findall(commit.message)\n",
    "\n",
    "        if len(matches) > 0:\n",
    "            commits_liked_with_issues += 1\n",
    "        for match in set(matches):\n",
    "            issue = graph.get_issue(match.upper())\n",
    "            edge = GitCommitIssueEdge(\n",
    "                git_commit = commit,\n",
    "                issue = issue,\n",
    "            )\n",
    "            graph.add_edge(edge)\n",
    "            links += 1\n",
    "\n",
    "    print(f\"There are {links} Issueâ€“Commit edges\")\n",
    "    print(f\"Commits liked with issues: {commits_liked_with_issues}\")\n",
    "\n",
    "def link_pull_request_with_issue(graph: Graph):\n",
    "    # Build one regex for all issue keys\n",
    "    issue_keys = [re.escape(issue.key) for issue in graph.issues.values()]\n",
    "    issue_pattern = re.compile(r'\\b(' + '|'.join(issue_keys) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    links = 0\n",
    "    prs_with_issues = 0\n",
    "\n",
    "    for pr in graph.pull_requests.values():\n",
    "        text = (pr.title or \"\") + \" \" + (pr.body or \"\")\n",
    "        matches = issue_pattern.findall(text)\n",
    "\n",
    "        if matches:\n",
    "            prs_with_issues += 1\n",
    "        for match in set(matches):\n",
    "            issue = graph.get_issue(match.upper())\n",
    "            if issue:\n",
    "                edge = PullRequestIssueEdge(\n",
    "                    pr=pr,\n",
    "                    issue=issue,\n",
    "                )\n",
    "                graph.add_edge(edge)\n",
    "                links += 1\n",
    "\n",
    "    print(f\"Created {links} PRâ€“Issue edges\")\n",
    "    print(f\"PRs linked with issues: {prs_with_issues}\")\n",
    "\n",
    "# Pre-sort commits by author_date\n",
    "all_commits = sorted(graph.commits.values(), key=lambda c: c.author_date)\n",
    "commit_dates = [c.author_date for c in all_commits]\n",
    "\n",
    "def candidates_in_window(target_date, days=14):\n",
    "    lo = bisect.bisect_left(commit_dates, target_date - timedelta(days=days))\n",
    "    hi = bisect.bisect_right(commit_dates, target_date + timedelta(days=days))\n",
    "    return all_commits[lo:hi]\n",
    "\n",
    "def clean(msg: str) -> str:\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "    msg = msg.strip().lower()\n",
    "\n",
    "    # remove PR references like \"(#123)\" or \"[#123]\"\n",
    "    msg = re.sub(r'\\(#\\d+\\)', '', msg)\n",
    "    msg = re.sub(r'\\[#\\d+\\]', '', msg)\n",
    "\n",
    "    # remove GitHub squash markers like \"squash commit\", \"merge pull request\"\n",
    "    msg = re.sub(r'merge pull request.*', '', msg)\n",
    "    msg = re.sub(r'squash commit.*', '', msg)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    msg = re.sub(r'\\s+', ' ', msg)\n",
    "\n",
    "    return msg.strip()\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def link_pull_requests_with_git_commits(graph: Graph, min_similarity = 0.85):\n",
    "    counts = {\n",
    "        \"merged_as\": 0,\n",
    "        \"contains_commit\": 0,\n",
    "        \"linked_via_issue\": 0\n",
    "    }\n",
    "    for pr in graph.pull_requests.values():\n",
    "        #TODO change with graph traversal functions\n",
    "        pr_commits = [c.commit for c in graph.adjacency.get(pr.dict_key(), {}).get(\"git_hub_commits\", [])]\n",
    "\n",
    "        for pr_commit in pr_commits:\n",
    "            # 1. Exact SHA match\n",
    "            if git_commit := graph.commits.get(f\"GitCommit:{pr_commit.sha}\"):\n",
    "                graph.add_edge(GitCommitPullRequestEdge(\n",
    "                    git_commit=git_commit,\n",
    "                    pr=pr,\n",
    "                    relation=\"merged_as\"\n",
    "                ))\n",
    "                counts[\"merged_as\"] += 1\n",
    "                continue\n",
    "\n",
    "            # 2 Fuzzy match within date window\n",
    "            candidates = candidates_in_window(pr_commit.date)\n",
    "            best_candidate = None\n",
    "            best_score = 0.0\n",
    "\n",
    "            for candidate in candidates:\n",
    "                score = similar(clean(pr_commit.message), clean(candidate.message))\n",
    "                if score > min_similarity and score > best_score:\n",
    "                    best_score = score\n",
    "                    best_candidate = candidate\n",
    "\n",
    "            if best_candidate:\n",
    "                graph.add_edge(GitCommitPullRequestEdge(\n",
    "                    git_commit=best_candidate,\n",
    "                    pr=pr,\n",
    "                    relation=\"contains_commit\",\n",
    "                ))\n",
    "                counts[\"contains_commit\"] += 1\n",
    "\n",
    "        # 3. Issue-based linking (fallback)\n",
    "        issues_linked_to_pr = [\n",
    "            edge.issue for edge in graph.adjacency.get(pr.dict_key(), {}).get(\"issues\", [])\n",
    "        ]\n",
    "        for issue in issues_linked_to_pr:\n",
    "            git_commits_for_issue = [\n",
    "                edge.git_commit for edge in graph.adjacency.get(issue.dict_key(), {}).get(\"git_commits\", [])\n",
    "            ]\n",
    "\n",
    "            existing_edges = set()\n",
    "\n",
    "            for git_commit in git_commits_for_issue:\n",
    "                edge = GitCommitPullRequestEdge(\n",
    "                    git_commit=git_commit,\n",
    "                    pr=pr,\n",
    "                    relation=\"linked_via_issue\"\n",
    "                )\n",
    "                if edge not in existing_edges:\n",
    "                    graph.add_edge(edge)\n",
    "                    counts[\"linked_via_issue\"] += 1\n",
    "                    existing_edges.add(edge)\n",
    "\n",
    "    for key in counts.keys():\n",
    "        print(f\"{key}: {counts[key]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_pickle(obj, var_name: str, base_dir: str = \"pickle_data\") -> Path:\n",
    "    \"\"\"\n",
    "    Save a Python object using pickle in a directory relative to the notebook.\n",
    "\n",
    "    Args:\n",
    "        obj: The Python object to save.\n",
    "        var_name: Name of the variable (used as filename).\n",
    "        base_dir: Relative directory to save pickles (default: \"pickle_data\").\n",
    "\n",
    "    Returns:\n",
    "        Path to the saved pickle file.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    save_dir = Path(base_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build full path\n",
    "    pickle_path = save_dir / f\"{var_name}.pkl\"\n",
    "\n",
    "    # Save object\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "    print(f\"Saved {var_name} to {pickle_path}\")\n",
    "    return pickle_path\n",
    "\n",
    "\n",
    "\n",
    "link_issues_with_git_commits(graph)\n",
    "link_pull_request_with_issue(graph)\n",
    "# 8 min 33 sec\n",
    "link_pull_requests_with_git_commits(graph)\n",
    "\n",
    "save_pickle(graph, \"graph\")"
   ],
   "id": "80b21a3fd419e3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3642 Issueâ€“Commit edges\n",
      "Commits liked with issues: 3209\n",
      "Created 4385 PRâ€“Issue edges\n",
      "PRs linked with issues: 3974\n",
      "merged_as: 916\n",
      "contains_commit: 2414\n",
      "linked_via_issue: 11182\n",
      "Saved graph to pickle_data/graph.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('pickle_data/graph.pkl')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ChatGPT web-browser",
   "id": "566dac40e48cf7d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T11:23:14.009365Z",
     "start_time": "2025-09-01T11:23:14.003972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Commit with most modified files\n",
    "\n",
    "most_modified = max(\n",
    "    graph.commits.values(),\n",
    "    key=lambda c: len(graph.adjacency.get(c.dict_key(), {}).get(\"files\", []))\n",
    ")\n",
    "print(most_modified.sha, most_modified.message)"
   ],
   "id": "fd88788c9c8fbbb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1c19ce2f393ca15e431e372b10e44163caa72d08 Merge pull request #87 from Leemoonsoo/master\n",
      "Rename zeppelin-web2 -> zeppelin-web\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T11:23:16.736684Z",
     "start_time": "2025-09-01T11:23:16.715369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Most seen files changes for issues of type \"Bug\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "bug_type = graph.get_issue_type(\"Bug\")\n",
    "\n",
    "file_counter = Counter()\n",
    "\n",
    "for issue in graph.issues.values():\n",
    "    if bug_type and any(\n",
    "        isinstance(e, IssueIssueTypeEdge) and e.issue_type.id == bug_type.id\n",
    "        for e in graph.adjacency.get(issue.dict_key(), {}).get(\"issue_types\", [])\n",
    "    ):\n",
    "        # get all commits linked to this issue\n",
    "        for edge in graph.adjacency.get(issue.dict_key(), {}).get(\"git_commits\", []):\n",
    "            commit = edge.git_commit\n",
    "            # get all files linked to this commit\n",
    "            for file_edge in graph.adjacency.get(commit.dict_key(), {}).get(\"files\", []):\n",
    "                file_counter[file_edge.file.path] += 1\n",
    "\n",
    "# print the top 5 most frequent files\n",
    "for file, count in file_counter.most_common(5):\n",
    "    print(f\"{file}: seen in {count} bug issues\")\n"
   ],
   "id": "a1a2d1f7b260b5fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dev/null: seen in 111 bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/socket/NotebookServer.java: seen in 97 bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/paragraph.controller.js: seen in 88 bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/Note.java: seen in 70 bug issues\n",
      "pom.xml: seen in 66 bug issues\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T11:23:19.546074Z",
     "start_time": "2025-09-01T11:23:19.519853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Find the bug type node (by name)\n",
    "bug_types = [\n",
    "    it for it in graph.issue_types.values()\n",
    "    if it.name == \"Bug\"\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Define traversal steps:\n",
    "#    IssueType â†’ Issues â†’ Commits â†’ Files\n",
    "steps = [\n",
    "    (\"issues\", None),                      # IssueType â†’ Issues\n",
    "    (\"git_commits\", None),                 # Issues â†’ Commits\n",
    "    (\"files\", None),                       # Commits â†’ Files\n",
    "]\n",
    "\n",
    "# 3. Traverse\n",
    "all_files = graph.filtered_traversal(\n",
    "    start_nodes=bug_types,\n",
    "    steps=steps,\n",
    ")\n",
    "\n",
    "# 4. Count by file path\n",
    "file_counter = Counter(f.path for f in all_files)\n",
    "\n",
    "# 5. Show top 5\n",
    "for file, count in file_counter.most_common(5):\n",
    "    print(f\"{file}: seen in {count} bug issues\")\n"
   ],
   "id": "2d5397d086e7d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dev/null: seen in 111 bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/socket/NotebookServer.java: seen in 97 bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/paragraph.controller.js: seen in 88 bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/Note.java: seen in 70 bug issues\n",
      "pom.xml: seen in 66 bug issues\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ai queries in code",
   "id": "f404220b0f57c86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T12:20:43.610934Z",
     "start_time": "2025-09-02T12:20:43.562028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1) Find the IssueType node for \"Bug\"\n",
    "bug_types = []\n",
    "get_type = getattr(graph, \"get_issue_type\", None)\n",
    "if callable(get_type):\n",
    "    bug_type = get_type(\"Bug\")\n",
    "    if bug_type:\n",
    "        bug_types = [bug_type]\n",
    "else:\n",
    "    bug_types = [it for it in graph.issue_types.values() if it.name.lower() == \"bug\"]\n",
    "\n",
    "if not bug_types:\n",
    "    print(\"No 'Bug' issue type found.\")\n",
    "else:\n",
    "    # 2) Traverse: IssueType -> Issues -> Commits\n",
    "    bug_commits = graph.filtered_traversal(\n",
    "        start_nodes=bug_types,\n",
    "        steps=[\n",
    "            (\"issues\", None),        # IssueType -> Issues\n",
    "            (\"git_commits\", None),   # Issues -> GitCommits\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 3) Deduplicate commits by SHA\n",
    "    unique_commits = {c.sha: c for c in bug_commits}\n",
    "\n",
    "    # 4) For each commit, collect files it touched\n",
    "    file_to_commits = defaultdict(set)  # file path -> set of commit SHAs\n",
    "    for commit in unique_commits.values():\n",
    "        for edge in graph.adjacency.get(commit.dict_key(), {}).get(\"files\", []):\n",
    "            file_node = edge.other_node(commit.dict_key())  # neighbor is the File node\n",
    "            file_to_commits[file_node.path].add(commit.sha)\n",
    "\n",
    "    # 5) Filter files with > 10 unique commits linked to Bug issues\n",
    "    files_gt_10 = {\n",
    "        path: shas for path, shas in file_to_commits.items()\n",
    "        if len(shas) > 10\n",
    "    }\n",
    "\n",
    "    # Optional: print sorted by number of commits descending\n",
    "    for path, shas in sorted(files_gt_10.items(), key=lambda kv: len(kv[1]), reverse=True):\n",
    "        print(f\"{path}: {len(shas)} commits linked to Bug issues\")"
   ],
   "id": "1116774d161df1f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeppelin-server/src/main/java/org/apache/zeppelin/socket/NotebookServer.java: 83 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/paragraph.controller.js: 75 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/Note.java: 66 commits linked to Bug issues\n",
      "pom.xml: 58 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/notebook.controller.js: 54 commits linked to Bug issues\n",
      "zeppelin-zengine/src/test/java/org/apache/zeppelin/notebook/NotebookTest.java: 52 commits linked to Bug issues\n",
      "jdbc/src/main/java/org/apache/zeppelin/jdbc/JDBCInterpreter.java: 48 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/Notebook.java: 47 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/server/ZeppelinServer.java: 41 commits linked to Bug issues\n",
      ".travis.yml: 40 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/interpreter/InterpreterSetting.java: 39 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/interpreter/InterpreterSettingManager.java: 39 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/rest/NotebookRestApi.java: 35 commits linked to Bug issues\n",
      "spark/src/main/java/org/apache/zeppelin/spark/SparkInterpreter.java: 35 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/Paragraph.java: 35 commits linked to Bug issues\n",
      "bin/interpreter.sh: 35 commits linked to Bug issues\n",
      "/dev/null: 33 commits linked to Bug issues\n",
      "zeppelin-web/src/components/navbar/navbar.controller.js: 29 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/interpreter/InterpreterFactory.java: 27 commits linked to Bug issues\n",
      "zeppelin-interpreter/src/main/java/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.java: 27 commits linked to Bug issues\n",
      "zeppelin-interpreter/src/main/java/org/apache/zeppelin/conf/ZeppelinConfiguration.java: 27 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/conf/ZeppelinConfiguration.java: 26 commits linked to Bug issues\n",
      "spark/pom.xml: 25 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/service/NotebookService.java: 25 commits linked to Bug issues\n",
      "zeppelin-zengine/pom.xml: 25 commits linked to Bug issues\n",
      "conf/zeppelin-site.xml.template: 25 commits linked to Bug issues\n",
      "zeppelin-server/pom.xml: 25 commits linked to Bug issues\n",
      "zeppelin-web/src/components/navbar/navbar.html: 25 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/rest/ZeppelinRestApiTest.java: 24 commits linked to Bug issues\n",
      "zeppelin-web/src/index.html: 24 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/result/result.controller.js: 24 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/notebook-actionBar.html: 23 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/rest/AbstractTestRestApi.java: 20 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/socket/NotebookServerTest.java: 19 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/rest/NotebookRestApiTest.java: 18 commits linked to Bug issues\n",
      "zeppelin-distribution/src/bin_license/LICENSE: 18 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/paragraph.css: 17 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/paragraph/paragraph.html: 16 commits linked to Bug issues\n",
      "python/src/main/java/org/apache/zeppelin/python/PythonInterpreter.java: 16 commits linked to Bug issues\n",
      "zeppelin-web/src/app/interpreter/interpreter.controller.js: 16 commits linked to Bug issues\n",
      "livy/src/main/java/org/apache/zeppelin/livy/LivySparkSQLInterpreter.java: 16 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/rest/ZeppelinSparkClusterTest.java: 15 commits linked to Bug issues\n",
      "spark-dependencies/pom.xml: 15 commits linked to Bug issues\n",
      "zeppelin-interpreter/src/main/java/org/apache/zeppelin/interpreter/remote/RemoteInterpreter.java: 15 commits linked to Bug issues\n",
      "zeppelin-web/src/app/notebook/notebook.html: 15 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/service/NotebookServiceTest.java: 15 commits linked to Bug issues\n",
      "spark/src/main/resources/python/zeppelin_pyspark.py: 15 commits linked to Bug issues\n",
      "zeppelin-web/package.json: 15 commits linked to Bug issues\n",
      "zeppelin-zengine/src/test/java/org/apache/zeppelin/notebook/repo/VFSNotebookRepoTest.java: 14 commits linked to Bug issues\n",
      "bin/common.sh: 14 commits linked to Bug issues\n",
      "spark/interpreter/pom.xml: 14 commits linked to Bug issues\n",
      "spark/interpreter/src/main/java/org/apache/zeppelin/spark/IPySparkInterpreter.java: 14 commits linked to Bug issues\n",
      "docs/setup/security/shiro_authentication.md: 14 commits linked to Bug issues\n",
      "spark/spark-scala-parent/src/main/scala/org/apache/zeppelin/spark/BaseSparkScalaInterpreter.scala: 14 commits linked to Bug issues\n",
      "zeppelin-web/src/app/interpreter/interpreter.html: 14 commits linked to Bug issues\n",
      "zeppelin-zengine/src/test/java/com/nflabs/zeppelin/zengine/stmt/ZQLTest.java: 14 commits linked to Bug issues\n",
      "zeppelin-web/pom.xml: 13 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/interpreter/launcher/SparkInterpreterLauncher.java: 13 commits linked to Bug issues\n",
      "conf/zeppelin-env.sh.template: 13 commits linked to Bug issues\n",
      "zeppelin-web/src/app/app.js: 13 commits linked to Bug issues\n",
      "jdbc/src/test/java/org/apache/zeppelin/jdbc/JDBCInterpreterTest.java: 13 commits linked to Bug issues\n",
      "flink/pom.xml: 13 commits linked to Bug issues\n",
      "zeppelin-server/src/test/java/org/apache/zeppelin/rest/InterpreterRestApiTest.java: 12 commits linked to Bug issues\n",
      "zeppelin-zengine/src/test/java/org/apache/zeppelin/notebook/repo/NotebookRepoSyncTest.java: 12 commits linked to Bug issues\n",
      "python/src/main/java/org/apache/zeppelin/python/IPythonInterpreter.java: 12 commits linked to Bug issues\n",
      "python/src/test/java/org/apache/zeppelin/python/IPythonInterpreterTest.java: 12 commits linked to Bug issues\n",
      "spark/src/main/java/org/apache/zeppelin/spark/PySparkInterpreter.java: 12 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/NoteManager.java: 12 commits linked to Bug issues\n",
      "zeppelin-web/src/components/websocketEvents/websocketEvents.factory.js: 12 commits linked to Bug issues\n",
      "zeppelin-web/bower.json: 12 commits linked to Bug issues\n",
      "spark/interpreter/src/main/java/org/apache/zeppelin/spark/PySparkInterpreter.java: 12 commits linked to Bug issues\n",
      "zeppelin-interpreter/pom.xml: 12 commits linked to Bug issues\n",
      "zeppelin-web/src/app/visualization/builtins/visualization-table.js: 12 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/rest/LoginRestApi.java: 12 commits linked to Bug issues\n",
      "zeppelin-zengine/src/test/java/org/apache/zeppelin/search/LuceneSearchTest.java: 11 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/repo/VFSNotebookRepo.java: 11 commits linked to Bug issues\n",
      "zeppelin-interpreter/src/main/java/org/apache/zeppelin/interpreter/InterpreterGroup.java: 11 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/rest/InterpreterRestApi.java: 11 commits linked to Bug issues\n",
      "python/src/test/java/org/apache/zeppelin/python/BasePythonInterpreterTest.java: 11 commits linked to Bug issues\n",
      "zeppelin-zengine/src/main/java/org/apache/zeppelin/notebook/repo/NotebookRepoSync.java: 11 commits linked to Bug issues\n",
      "zeppelin-web/src/app/home/home.css: 11 commits linked to Bug issues\n",
      "zeppelin-web/src/app/home/home.html: 11 commits linked to Bug issues\n",
      "jdbc/src/main/resources/interpreter-setting.json: 11 commits linked to Bug issues\n",
      "zeppelin-server/src/main/java/org/apache/zeppelin/utils/SecurityUtils.java: 11 commits linked to Bug issues\n",
      "zeppelin-web/src/app/home/home.controller.js: 11 commits linked to Bug issues\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
